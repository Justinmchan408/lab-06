---
title: "lab-06: Model Selection + Diagnostics"
author: "Justin Chan"
date: "2/21/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

Loading required packages for the lab:
```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(broom)
library(leaps)
library(rms)
library(Sleuth3)
```

## Exercises

### Part I: Model Selection

We begin this lab by conducting model selection with various selection criteria to choose a final model from the SAT dataset. The code to load the data and create the full main effects model is shown below. The next few questions will walk you through backward model selection using different model selection criteria to select a model.

```{r dataset, include=TRUE}
sat_scores <- Sleuth3::case1201

full_model <- lm(SAT ~ Takers + Income + Years + Public + Expend + Rank, data = sat_scores)
tidy(full_model)
```

1. We will use the regsubsets function in the leaps R package to perform backward selection on multiple linear regression models with Adj.R2 or BIC as the selection criteria.

```{r regsubsets, include=TRUE}
model_select <- regsubsets(SAT ~ Takers + Income + Years + Public + Expend + Rank , data = sat_scores, method = "backward")
select_summary <- summary(model_select)
coef(model_select, 1:6)
select_summary$adjr2
```

2. Fill in the code below to display the model selected from backward selection with BIC as the selection criterion.
```{r bic, include=TRUE}
select_summary$bic
```

3. Next, let’s select a model using AIC as the selection criterion. To select a model using AIC, we will use the step function in R. The code below is to conduct backward selection using AIC as the criterion and store the selected model in an object called model_select_aic. Use the tidy function to display the coefficients of the selected model.

```{r aic, include=TRUE}
model_select_aic <- step(full_model, direction = "backward")
tidy(model_select_aic)
```

4 Compare the final models selected by Adj.R2, AIC, and BIC.
- Do the models have the same number of predictors?
- If they don’t have the same number of predictors, which selection criterion resulted in the model with the fewest number of predictors? Is this what you would expect? Briefly explain.

The models do not have the same predictors where the BIC has three predictors and the AIC and Adj.R2 have four predictors based on the models created above. The selection criterion that resulted in the fewest number of parameters is BIC with three predictors. No, this is not what I would expect completely. I would expect AIC and BIC would have fewer predictors since they help determine the best possible model without overpredicting or adding more variables that may add more noise than help by factoring the log likelihood. However, BIC only had the smallest amount predictors based on the smallest BIC value.

### Part II: Model Diagnostics

Let’s choose model_select_aic, the model selected usng AIC, to be our final model. In this part of the lab, we will examine some model diagnostics for this model.

5. Use the augment function to create a data frame that contains model predictiosn and statistics for each observation. Save the data frame, and add a variable called obs_num that contains the observation (row) number. Display the first 5 rows of the new data frame.

```{r data frame, include=TRUE}
df_model_prediction_statistics <- augment(model_select_aic)
df_model_prediction_statistics <- df_model_prediction_statistics %>% 
  mutate(obs_num = row_number())
head(df_model_prediction_statistics, 5)
```

6. Let’s examine the leverage for each observation. Based on the lecture notes, what threshold should we use to determine if observations in this dataset have high leverage? Report the value and show the quation you used to calculate it.

The threshold we should use to determine if the observations in this dataset have high leverage if its over 2 times the average leverage for all observations from the lecture. The equation to calculate the threshold:
$$h_i > \frac{2(p+1)}{n} > \frac{2(4+1)}{50} > 0.2$$

7. Plot the leverage (.hat) vs. the observation number. Add a line on the plot marking the threshold from the previous exercise. Be sure to include an informative title and clearly label the axes. You can use geom_hline to the add the threshold line to the plot.

```{r leverage vs obs num, include=TRUE}
leverage_threshold <- 2*(4+1)/nrow(df_model_prediction_statistics)

ggplot(data = df_model_prediction_statistics, aes(x = obs_num, y = .hat)) +
  geom_point() + 
  geom_hline(aes(yintercept = leverage_threshold, colour = "red"))
```

8. Which states (if any) in the dataset are considered high leverage? Show the code used to determine the states. Hint: You may need to get State from sat_data.

```{r high leverage states, include=TRUE}
df_model_prediction_statistics %>% filter(.hat > leverage_threshold) %>% 
  select(obs_num, Years, Public, Expend, Rank)
```
The states in the dataset that are considered high leverage is Louisiana and Alaska which are the rows 22 and 29 in the sat_scores dataset.

9. Next, we will examine the standardized residuals. Plot the standardized residuals (.std.resid) versus the predicted values. Include horizontal lines at y=2 and y=-2 indicating the thresholds used to determine if standardized residuals have a large magnitude. Be sure to include an informative title and clearly label the axes.You can use geom_hline to the add the threshold lines to the plot.

```{r std residuals vs predicted values, include=TRUE}
ggplot(data = df_model_prediction_statistics, aes(x = .fitted, y = .std.resid)) +
  geom_point() + 
  geom_hline(aes(yintercept = 2, colour = "red")) +
  geom_hline(aes(yintercept = -2, colour = "red")) +
  labs(title = "Standardized Residuals vs Predicted Values",
       x = "Predicted Values",
       y = "Standardized Residuals")
```

10. Based on our thresholds, which states (if any) are considered to have standardized residuals with large magnitude? Show the code used to determine the states. Hint: You may need to get State from sat_data.

```{r large magnitude filter, include=TRUE}
df_model_prediction_statistics %>% filter(2 < abs(.std.resid)) %>% 
  select(obs_num, Years, Public, Expend, Rank)
```

The states in the dataset that are considered to have standardized residuals with large magnitude are Mississippi, Alaska, and North Carolina which are the rows 16, 29 and 50 in the sat_scores dataset.

11. Let’s determine if any of these states with high leverage and/or high standardized residuals are influential points, i.e. are significantly impacting the coefficients of the model. Plot the Cook’s Distance (.cooksd) vs. the observation number. Add a line on the plot marking the threshold to determine a point is influential. Be sure to include an informative title and clearly label the axes. You can use geom_hline to the add the threshold line to the plot.
- Which states (if any) are considered to be influential points?
- If there are influential points, briefly describe strategies to deal with them in your regression analysis.

```{r cooks distance vs obs num, include=TRUE}
ggplot(data = df_model_prediction_statistics, aes(x = obs_num, y = .cooksd)) +
  geom_point(alpha = 0.7) + 
  geom_hline(aes(yintercept = 1, colour = "red")) +
  labs(title = "Cook's Distance vs Observation Number",
       x = "Observation Number",
       y = "Cook's Distance")

df_model_prediction_statistics %>% filter(1 < .cooksd) %>% 
  select(obs_num, Years, Public, Expend, Rank)
```

The state in the dataset that are considered to be influential point is Alaska which is row 29 in the sat_scores dataset. One strategy to deal with these influencial points or ways to drop the point based on predictor variables if it is meaningful to drop the observation given the context of the problem, build a model with a smaller range of predictor variables and mention this in the write up. Other strategies to deal with these influential points is transformations or increasing the sample size by collecting more data.

12. Lastly, let’s examine the Variance Inflation Factor (VIF) used to determine if the predictor variables in the model are correlated with each other.

Let’s start by manually calculating VIF for the variable Expend.
- Begin by fitting a model with Expend as the response variable and the other predictor variables in model_select_aic as the predictors.
- Calculate R2 for this model.
- Use this R2 to calculate VIF for Expend.
- Does Expend appear to be highly correlated with any other predictor variables? Briefly explain.

```{r expend model, include=TRUE}
expend_model <- lm(Expend ~ Years + Public + Rank, data = sat_scores)
summary(expend_model)$r.squared
```
$$VIF = \frac{1}{1 - R^2_{expend}} = \frac{1}{1 - 0.2102} = 1.27 $$
The Expend does not appearly to be highly correlated with any other predictor variables since VIF value, 1.26 is not large enough to show mutlicollinearity with other variables. Often, VIF values over 10 indicate concerning mutlicollinearity where there is high correlation between two or more explanatory variables which often occurs in smaller sample sizes. In this model, mutlicollinearity does not seem to be an issue with the variable extend that has a VIF much smaller than 10 as previously mentioned.

12. Now, let’s use the vif function in the rms package to calculate VIF for all of the variables in the model. You can use the tidy function to output the results neatly in a data frame. Are there any obvious concerns with multicollinearity in this model? Briefly explain.

```{r vif, include=TRUE}
tidy(vif(model_select_aic))
```

No, there does not seem an issue of multicollinearity in this model, model_select_aic after using the function vif() from the rms package which has all the VIF explanatory values in the model to be less than 1.43. If any of these VIF values were over 10, then we would most likely have a problem with mutlicollinearity but, there does not seem to be these obvious concerns at this first glance/analysis of the model.

```{r write raw and mod data, include=FALSE}
write.csv(sat_scores, file = "/Users/chanj4/Desktop/School/STAT108/lab-06/raw_data/sat_scores.csv")

write.csv(df_model_prediction_statistics, file = "/Users/chanj4/Desktop/School/STAT108/lab-06/mod_data/df_model_prediction_statistics.csv")
```

